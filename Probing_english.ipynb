{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9503bc",
   "metadata": {},
   "source": [
    "# Probing of the English markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df032d41-ce43-447c-b447-ad3ec688735b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bec3151-5561-46ee-b14c-16121db0a1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import load_files_from_folder\n",
    "from utils import separate_target_and_non_target\n",
    "from utils import balance_and_create_layered_data\n",
    "from utils import run_probing_on_all_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa40dd3-2494-4a3e-9ffb-b8f3d309ef5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "markers=['Accusative',\n",
    "         'Causal_final', \n",
    "         'Dative', \n",
    "         'Genitive',\n",
    "         'Plural',\n",
    "         'Sublative',\n",
    "         'Translative',\n",
    "         'VerbConjugation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ffb6762-30ef-41f9-8b58-0f1c692aed79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_label_data(data_folder):\n",
    "    # Get all subfolder names in the provided directory path\n",
    "    all_marker_names = set()\n",
    "    labeled_dfs = {}\n",
    "    \n",
    "    # Step 1: Identify all marker types based on folder names\n",
    "    for root, dirs, files in os.walk(data_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\"representations.csv\"):\n",
    "                # Extract marker name from the folder name\n",
    "                marker_name = os.path.basename(root).split(\"_representations\")[0]\n",
    "                all_marker_names.add(marker_name)\n",
    "\n",
    "    # Step 2: Process each file and assign labels\n",
    "    for root, dirs, files in os.walk(data_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\"representations.csv\"):\n",
    "                # Extract marker name\n",
    "                marker_name = os.path.basename(root).split(\"_representations\")[0]\n",
    "                \n",
    "                # Load the data file\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Add a column for each marker, setting to 0 by default\n",
    "                for marker in all_marker_names:\n",
    "                    df[marker] = 0\n",
    "                \n",
    "                # Set the current marker column to 1 (indicating presence)\n",
    "                df[marker_name] = 1\n",
    "                \n",
    "                # Append to the dictionary, concatenating if marker already exists\n",
    "                if marker_name in labeled_dfs:\n",
    "                    labeled_dfs[marker_name] = pd.concat([labeled_dfs[marker_name], df], ignore_index=True)\n",
    "                else:\n",
    "                    labeled_dfs[marker_name] = df\n",
    "\n",
    "    return labeled_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46dcb21-6f59-43c9-80f3-95ae7b416586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labeled_dfs=load_and_label_data('Full_data_english/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8cd715c-9231-4ac7-a870-145879b347e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def preprocess_representation(representation_str):\n",
    "    \"\"\"\n",
    "    Converts a string representation of a list of NumPy arrays into lists of exactly 768 floats each.\n",
    "    Ensures exactly 12 layers by removing any extraneous information and handling inconsistencies.\n",
    "\n",
    "    Args:\n",
    "        representation_str (str): String representation of a list of NumPy arrays.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing exactly 12 lists of 768 floats, each representing a layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Strip off the initial and final parts\n",
    "    stripped_str = representation_str.lstrip('[array(').rstrip('])')\n",
    "\n",
    "    # Step 2: Split by '), array([' to separate each layer\n",
    "    layer_strs = stripped_str.split('), array([')\n",
    "\n",
    "    # Debug: Print each identified layer for verification\n",
    "    # print(\"Identified layers (raw strings):\")\n",
    "    # for i, layer in enumerate(layer_strs):\n",
    "        # print(f\"Layer {i + 1}: {layer[:100]}...\")  # Print first 100 chars for each layer\n",
    "        # print(f\"Length of Layer {i + 1} (raw): {len(layer.split(','))}\")  # Print the raw number of elements\n",
    "\n",
    "    # Step 3: Convert each layer to a list of floats, handling any dtype metadata\n",
    "    processed_layers = []\n",
    "    for idx, layer_str in enumerate(layer_strs):\n",
    "        # Stop processing if more than 12 layers have been processed\n",
    "        \n",
    "\n",
    "        # Remove any dtype information if present in the 13th or other unexpected layers\n",
    "        clean_layer_str = layer_str.split('\\ndtype=float32')[0].strip()\n",
    "        clean_layer_str = clean_layer_str.replace(' ', '').replace('[', '').replace(']', '')\n",
    "        clean_layer_str = clean_layer_str.replace('\\ndtype=float32', '')\n",
    "        clean_layer_str = clean_layer_str.replace('dtype=float32','')\n",
    "        # Parse floats and take only the first 768 elements\n",
    "        float_values = [float(val) for val in clean_layer_str.split(\",\") if val][:768]\n",
    "\n",
    "        # Ensure we have exactly 768 elements in each layer\n",
    "        if len(float_values) == 768:\n",
    "            processed_layers.append(float_values)\n",
    "\n",
    "            # print(f\"Warning: Layer with unexpected length {len(float_values)} found, discarding it.\")\n",
    "\n",
    "    # Step 4: Handle cases where there are more or fewer than 12 layers\n",
    "    if len(processed_layers) < 12:\n",
    "        # print(f\"Warning: Expected 12 layers but found {len(processed_layers)}. Adding empty layers.\")\n",
    "        processed_layers.extend([[0.0] * 768] * (12 - len(processed_layers)))\n",
    "\n",
    "    return processed_layers\n",
    "\n",
    "   # Return 12 layers with 768 zeroes in case of error\n",
    "\n",
    "def transform_to_hungarian_structure(df):\n",
    "    \"\"\"\n",
    "    Transforms the input DataFrame to match the structure of the Hungarian dataset for probing experiments.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to transform (e.g., English dataset).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed DataFrame with the same structure as the Hungarian dataset.\n",
    "    \"\"\"\n",
    "    # Rename columns to match the Hungarian dataset\n",
    "    df = df.rename(columns={'Causal_final': 'CausalFinal'})\n",
    "    \n",
    "    # Add missing columns with default values of 0\n",
    "    required_columns = ['Accusative', 'Genitive', 'Dative', 'Sublative', 'CausalFinal', 'Translative', 'Plural', 'VerbConjugation']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    # Preprocess 'Hidden Representations (All Layers)' column\n",
    "    df['Hidden Representations (All Layers)'] = df['Hidden Representations (All Layers)'].apply(preprocess_representation)\n",
    "\n",
    "    # Reorder columns to match the Hungarian dataset\n",
    "    column_order = ['Word', 'Lemma', 'Sentence', 'Hidden Representations (All Layers)', \n",
    "                    'Accusative', 'Genitive', 'Dative', 'Sublative', 'CausalFinal', \n",
    "                    'Translative', 'Plural', 'VerbConjugation']\n",
    "    df = df[column_order]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e07718d-cd89-4f29-af8e-5c72577d265f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "representations={}\n",
    "for key, value in labeled_dfs.items():\n",
    "    df=transform_to_hungarian_structure(value)\n",
    "    representations[key]=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086629a3-2808-4cc4-9590-70038b909c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 representations saved for train and eval datasets in 'Layered_data_english/Accusative/1'.\n",
      "Layer 2 representations saved for train and eval datasets in 'Layered_data_english/Accusative/2'.\n",
      "Layer 3 representations saved for train and eval datasets in 'Layered_data_english/Accusative/3'.\n",
      "Layer 4 representations saved for train and eval datasets in 'Layered_data_english/Accusative/4'.\n",
      "Layer 5 representations saved for train and eval datasets in 'Layered_data_english/Accusative/5'.\n",
      "Layer 6 representations saved for train and eval datasets in 'Layered_data_english/Accusative/6'.\n",
      "Layer 7 representations saved for train and eval datasets in 'Layered_data_english/Accusative/7'.\n",
      "Layer 8 representations saved for train and eval datasets in 'Layered_data_english/Accusative/8'.\n",
      "Layer 9 representations saved for train and eval datasets in 'Layered_data_english/Accusative/9'.\n",
      "Layer 10 representations saved for train and eval datasets in 'Layered_data_english/Accusative/10'.\n",
      "Layer 11 representations saved for train and eval datasets in 'Layered_data_english/Accusative/11'.\n",
      "Layer 12 representations saved for train and eval datasets in 'Layered_data_english/Accusative/12'.\n",
      "Layer 1 representations saved for train and eval datasets in 'Layered_data_english/Causal_final/1'.\n",
      "Layer 2 representations saved for train and eval datasets in 'Layered_data_english/Causal_final/2'.\n",
      "Layer 3 representations saved for train and eval datasets in 'Layered_data_english/Causal_final/3'.\n",
      "Layer 4 representations saved for train and eval datasets in 'Layered_data_english/Causal_final/4'.\n",
      "Layer 5 representations saved for train and eval datasets in 'Layered_data_english/Causal_final/5'.\n",
      "Layer 6 representations saved for train and eval datasets in 'Layered_data_english/Causal_final/6'.\n",
      "Layer 7 representations saved for train and eval datasets in 'Layered_data_english/Causal_final/7'.\n",
      "Layer 8 representations saved for train and eval datasets in 'Layered_data_english/Causal_final/8'.\n",
      "Layer 9 representations saved for train and eval datasets in 'Layered_data_english/Causal_final/9'.\n",
      "Layer 10 representations saved for train and eval datasets in 'Layered_data_english/Causal_final/10'.\n",
      "Layer 11 representations saved for train and eval datasets in 'Layered_data_english/Causal_final/11'.\n",
      "Layer 12 representations saved for train and eval datasets in 'Layered_data_english/Causal_final/12'.\n",
      "Layer 1 representations saved for train and eval datasets in 'Layered_data_english/Dative/1'.\n",
      "Layer 2 representations saved for train and eval datasets in 'Layered_data_english/Dative/2'.\n",
      "Layer 3 representations saved for train and eval datasets in 'Layered_data_english/Dative/3'.\n",
      "Layer 4 representations saved for train and eval datasets in 'Layered_data_english/Dative/4'.\n",
      "Layer 5 representations saved for train and eval datasets in 'Layered_data_english/Dative/5'.\n",
      "Layer 6 representations saved for train and eval datasets in 'Layered_data_english/Dative/6'.\n",
      "Layer 7 representations saved for train and eval datasets in 'Layered_data_english/Dative/7'.\n",
      "Layer 8 representations saved for train and eval datasets in 'Layered_data_english/Dative/8'.\n",
      "Layer 9 representations saved for train and eval datasets in 'Layered_data_english/Dative/9'.\n",
      "Layer 10 representations saved for train and eval datasets in 'Layered_data_english/Dative/10'.\n",
      "Layer 11 representations saved for train and eval datasets in 'Layered_data_english/Dative/11'.\n",
      "Layer 12 representations saved for train and eval datasets in 'Layered_data_english/Dative/12'.\n",
      "Layer 1 representations saved for train and eval datasets in 'Layered_data_english/Genitive/1'.\n",
      "Layer 2 representations saved for train and eval datasets in 'Layered_data_english/Genitive/2'.\n",
      "Layer 3 representations saved for train and eval datasets in 'Layered_data_english/Genitive/3'.\n",
      "Layer 4 representations saved for train and eval datasets in 'Layered_data_english/Genitive/4'.\n",
      "Layer 5 representations saved for train and eval datasets in 'Layered_data_english/Genitive/5'.\n",
      "Layer 6 representations saved for train and eval datasets in 'Layered_data_english/Genitive/6'.\n",
      "Layer 7 representations saved for train and eval datasets in 'Layered_data_english/Genitive/7'.\n",
      "Layer 8 representations saved for train and eval datasets in 'Layered_data_english/Genitive/8'.\n",
      "Layer 9 representations saved for train and eval datasets in 'Layered_data_english/Genitive/9'.\n",
      "Layer 10 representations saved for train and eval datasets in 'Layered_data_english/Genitive/10'.\n",
      "Layer 11 representations saved for train and eval datasets in 'Layered_data_english/Genitive/11'.\n",
      "Layer 12 representations saved for train and eval datasets in 'Layered_data_english/Genitive/12'.\n",
      "Layer 1 representations saved for train and eval datasets in 'Layered_data_english/Plural/1'.\n",
      "Layer 2 representations saved for train and eval datasets in 'Layered_data_english/Plural/2'.\n",
      "Layer 3 representations saved for train and eval datasets in 'Layered_data_english/Plural/3'.\n",
      "Layer 4 representations saved for train and eval datasets in 'Layered_data_english/Plural/4'.\n",
      "Layer 5 representations saved for train and eval datasets in 'Layered_data_english/Plural/5'.\n",
      "Layer 6 representations saved for train and eval datasets in 'Layered_data_english/Plural/6'.\n",
      "Layer 7 representations saved for train and eval datasets in 'Layered_data_english/Plural/7'.\n",
      "Layer 8 representations saved for train and eval datasets in 'Layered_data_english/Plural/8'.\n",
      "Layer 9 representations saved for train and eval datasets in 'Layered_data_english/Plural/9'.\n",
      "Layer 10 representations saved for train and eval datasets in 'Layered_data_english/Plural/10'.\n",
      "Layer 11 representations saved for train and eval datasets in 'Layered_data_english/Plural/11'.\n",
      "Layer 12 representations saved for train and eval datasets in 'Layered_data_english/Plural/12'.\n",
      "Layer 1 representations saved for train and eval datasets in 'Layered_data_english/Sublative/1'.\n",
      "Layer 2 representations saved for train and eval datasets in 'Layered_data_english/Sublative/2'.\n",
      "Layer 3 representations saved for train and eval datasets in 'Layered_data_english/Sublative/3'.\n",
      "Layer 4 representations saved for train and eval datasets in 'Layered_data_english/Sublative/4'.\n",
      "Layer 5 representations saved for train and eval datasets in 'Layered_data_english/Sublative/5'.\n",
      "Layer 6 representations saved for train and eval datasets in 'Layered_data_english/Sublative/6'.\n",
      "Layer 7 representations saved for train and eval datasets in 'Layered_data_english/Sublative/7'.\n",
      "Layer 8 representations saved for train and eval datasets in 'Layered_data_english/Sublative/8'.\n",
      "Layer 9 representations saved for train and eval datasets in 'Layered_data_english/Sublative/9'.\n",
      "Layer 10 representations saved for train and eval datasets in 'Layered_data_english/Sublative/10'.\n",
      "Layer 11 representations saved for train and eval datasets in 'Layered_data_english/Sublative/11'.\n",
      "Layer 12 representations saved for train and eval datasets in 'Layered_data_english/Sublative/12'.\n",
      "Layer 1 representations saved for train and eval datasets in 'Layered_data_english/Translative/1'.\n",
      "Layer 2 representations saved for train and eval datasets in 'Layered_data_english/Translative/2'.\n",
      "Layer 3 representations saved for train and eval datasets in 'Layered_data_english/Translative/3'.\n",
      "Layer 4 representations saved for train and eval datasets in 'Layered_data_english/Translative/4'.\n",
      "Layer 5 representations saved for train and eval datasets in 'Layered_data_english/Translative/5'.\n",
      "Layer 6 representations saved for train and eval datasets in 'Layered_data_english/Translative/6'.\n",
      "Layer 7 representations saved for train and eval datasets in 'Layered_data_english/Translative/7'.\n",
      "Layer 8 representations saved for train and eval datasets in 'Layered_data_english/Translative/8'.\n",
      "Layer 9 representations saved for train and eval datasets in 'Layered_data_english/Translative/9'.\n",
      "Layer 10 representations saved for train and eval datasets in 'Layered_data_english/Translative/10'.\n",
      "Layer 11 representations saved for train and eval datasets in 'Layered_data_english/Translative/11'.\n",
      "Layer 12 representations saved for train and eval datasets in 'Layered_data_english/Translative/12'.\n",
      "Layer 1 representations saved for train and eval datasets in 'Layered_data_english/VerbConjugation/1'.\n",
      "Layer 2 representations saved for train and eval datasets in 'Layered_data_english/VerbConjugation/2'.\n",
      "Layer 3 representations saved for train and eval datasets in 'Layered_data_english/VerbConjugation/3'.\n",
      "Layer 4 representations saved for train and eval datasets in 'Layered_data_english/VerbConjugation/4'.\n",
      "Layer 5 representations saved for train and eval datasets in 'Layered_data_english/VerbConjugation/5'.\n",
      "Layer 6 representations saved for train and eval datasets in 'Layered_data_english/VerbConjugation/6'.\n",
      "Layer 7 representations saved for train and eval datasets in 'Layered_data_english/VerbConjugation/7'.\n",
      "Layer 8 representations saved for train and eval datasets in 'Layered_data_english/VerbConjugation/8'.\n",
      "Layer 9 representations saved for train and eval datasets in 'Layered_data_english/VerbConjugation/9'.\n",
      "Layer 10 representations saved for train and eval datasets in 'Layered_data_english/VerbConjugation/10'.\n",
      "Layer 11 representations saved for train and eval datasets in 'Layered_data_english/VerbConjugation/11'.\n",
      "Layer 12 representations saved for train and eval datasets in 'Layered_data_english/VerbConjugation/12'.\n"
     ]
    }
   ],
   "source": [
    "for target_marker in markers:\n",
    "    target_reps, non_target_reps=separate_target_and_non_target(representations, target_marker)\n",
    "    balance_and_create_layered_data(target_reps, non_target_reps, target_marker, layered_data_folder='Layered_data_english', target_threshold=200, eval_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3b37f6-87e9-4f8e-8b6e-ac521b69fd89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing marker: Accusative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/g/Documents/School_stuff/Master/Experiments_NLP/probing_resources/utils.py:569: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cumulative_results_df = pd.concat([cumulative_results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative results for all layers of Accusative saved to Output_comparison_english/Accusative/Accusative_results.csv\n",
      "Processing marker: Plural\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/g/Documents/School_stuff/Master/Experiments_NLP/probing_resources/utils.py:569: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cumulative_results_df = pd.concat([cumulative_results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative results for all layers of Plural saved to Output_comparison_english/Plural/Plural_results.csv\n",
      "Processing marker: Translative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/g/Documents/School_stuff/Master/Experiments_NLP/probing_resources/utils.py:569: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cumulative_results_df = pd.concat([cumulative_results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative results for all layers of Translative saved to Output_comparison_english/Translative/Translative_results.csv\n",
      "Processing marker: Dative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/g/Documents/School_stuff/Master/Experiments_NLP/probing_resources/utils.py:569: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cumulative_results_df = pd.concat([cumulative_results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative results for all layers of Dative saved to Output_comparison_english/Dative/Dative_results.csv\n",
      "Processing marker: Causal_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/g/Documents/School_stuff/Master/Experiments_NLP/probing_resources/utils.py:569: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cumulative_results_df = pd.concat([cumulative_results_df, pd.DataFrame([{\n",
      "/Users/g/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/g/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative results for all layers of Causal_final saved to Output_comparison_english/Causal_final/Causal_final_results.csv\n",
      "Processing marker: Sublative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/g/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/g/Documents/School_stuff/Master/Experiments_NLP/probing_resources/utils.py:569: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cumulative_results_df = pd.concat([cumulative_results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative results for all layers of Sublative saved to Output_comparison_english/Sublative/Sublative_results.csv\n",
      "Processing marker: VerbConjugation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/g/Documents/School_stuff/Master/Experiments_NLP/probing_resources/utils.py:569: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cumulative_results_df = pd.concat([cumulative_results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative results for all layers of VerbConjugation saved to Output_comparison_english/VerbConjugation/VerbConjugation_results.csv\n",
      "Processing marker: Genitive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/g/Documents/School_stuff/Master/Experiments_NLP/probing_resources/utils.py:569: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  cumulative_results_df = pd.concat([cumulative_results_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative results for all layers of Genitive saved to Output_comparison_english/Genitive/Genitive_results.csv\n"
     ]
    }
   ],
   "source": [
    "run_probing_on_all_layers(layered_data_folder='Layered_data_english', output_folder='Output_comparison_english')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
